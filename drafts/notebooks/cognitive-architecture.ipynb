{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Degree of autonomy of an LLM application\n",
    "1. Have an LLM decide the output of a step.\n",
    "2. Have an LLM decide the next step to take.\n",
    "3. Have an LLM decide what steps are available to take."
   ],
   "id": "c61af664a2e7b04c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cognitive architecture levels\n",
    "An LLM cognitive architecture can be defined as a recipe for the steps to be taken by an LLM application.\n",
    "\n",
    "* 0-3: Human driven\n",
    "* 4-7: Agent\n",
    "\n",
    "0. Code: Not an LLM cognitive architecture. Just regular software.\n",
    "1. LLM call: Makes use of an LLM for achieving a specific task, say translating or summarizing a piece of text.\n",
    "2. Chain: Use of multiple LLM calls in a predefined sequence (static sequence steps). Ex: text-to-SQL, first LLM call to generate a SQL query from NL query and database\n",
    "contents provided by dev, then another LLM call to write an explanation of the query appropriate for a nontechnical user.\n",
    "3. Router: LLM define what steps to take (conditionally), these steps is predefined by dev. Ex, RAG , tool call,... using an LLM to evaluate each\n",
    "incoming query and decide which index (tool, document) it should use for that particular query. Before the advent of LLMs,\n",
    "the usual way of solving this problem would be to build a classifier model.\n",
    "\n"
   ],
   "id": "21f560b07356cffb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Agent architectures\n",
    "Agent is \"something that acts\". Act details:\n",
    "* Acting requires some capacity for deciding what to do.\n",
    "* Deciding what to do implies having access to more than one possible course of action. After all, a decision without options is no decision at all.\n",
    "* In order to decide, the agent also needs access to information about the external environment (anything outside of the agent itself).\n",
    "\n",
    "So an agentic LLM application must be one that uses an LLM to pick from one or more possible courses of action. Given some context about\n",
    " the current state of the world or some desired next state, these attributes are usually implemented by mixing two prompting techniques:\n",
    " Tool calling and Chain-of-thought.\n",
    "\n",
    "What makes the agent architecture different from the architectures discussed above is concept of the LLM-driven loop: Planning an action(s)\n",
    "and Executing said action(s). This is call ReAct. (see more in graph.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d657689ef1c34f1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Always Calling a Tool First\n",
    "In the standard agent architecture, the LLM is always called upon to decide what tool to call next. It gives the LLM ultimate flexibility to adapt the behavior of the application to each user query that comes in. But it come with a cost of unpredictability.\n",
    "\n",
    "Some time dev know that search tool should always be called first, as it will skip the first LLM call and prevent the LLM from erroneously deciding it doesnâ€™t need to call the tools. So do this if your prompt always have something like \"call search tool first then answer\"."
   ],
   "id": "20ac6cd28207c94b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dealing with many tools\n",
    "When given many tools (say, more than 10) the planning performance (that is, choosing the right tool) starts to suffer.\n",
    "* Solutions: Use a RAG step to preselect the most relevant tools.\n",
    "\n",
    "\n"
   ],
   "id": "2ddf5059ee309b9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reflection aka self-critique\n",
    "Creation of a loop between a creator prompt and a reviser prompt. We create two nodes, generate and reflect. We can run loop in fix times or\n",
    "let reflect node decide when to finish.\n",
    "> If you were writing a code-generation agent, you could have a step before reflect that would run the code through a linter or compiler and report any errors as input to reflect.\n"
   ],
   "id": "5d39b3a153569f96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multi-agents\n",
    "Architects:\n",
    "1. Network: Each agent can communicate with every other agent. Any agent can decide which other agent is to be executed next.\n",
    "2. Supervisor: Each agent communicates with a single agent, called the supervisor, called the supervisor. The supervisor agent makes decisions on which agent (or agents) should be called next.\n",
    "3. Hierarchical: Supervisor of supervisors\n",
    "4. Custom"
   ],
   "id": "da0e4bf90945216c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Design keys\n",
    "1. Streaming/intermediate output\n",
    "2. Structured output: Low temperature is usually a good fit for that.\n",
    "3. Human in the loop\n",
    "4. Double texting modes"
   ],
   "id": "38f4d30c2cfb3ecf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1a9360979e199b61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
