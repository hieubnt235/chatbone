{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Models in LangChain\n",
    "LangChain provide 2 interfaces to interact with any LLM API Provider.\n",
    "1. LLMs (base: BaseLLM): take a string in and output a string.\n",
    "2. Chat models (base: BaseChatModel): newer forms of language models that take messages in and output a message."
   ],
   "id": "86b6bc8dfd750adc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LLMs",
   "id": "30e51f72b12a9349"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-06T12:58:45.797238Z",
     "start_time": "2025-02-06T12:58:34.749574Z"
    }
   },
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig,Qwen2TokenizerFast, LlamaTokenizer\n",
    "from transformers import Qwen2ForCausalLM, set_seed, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model_path = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "\n",
    "quantization_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             # torch_dtype=torch.bfloat16,\n",
    "                                             quantization_config=quantization_cfg,\n",
    "                                             low_cpu_mem_usage=True\n",
    "                                             )\n",
    "assert isinstance(model, Qwen2ForCausalLM)\n",
    "hf_pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer, device_map='auto',\n",
    "                   max_new_tokens=500)\n",
    "assert isinstance(hf_pipe, TextGenerationPipeline)\n",
    "# HuggingFacePipeline is BaseLLM\n",
    "pipeline = HuggingFacePipeline(pipeline=hf_pipe)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T15:57:01.270112Z",
     "start_time": "2025-02-06T15:57:01.267262Z"
    }
   },
   "cell_type": "code",
   "source": "type(tokenizer)",
   "id": "3f9dbd380f5d1758",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:58:50.595043Z",
     "start_time": "2025-02-06T12:58:47.589282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline.invoke(\"The sky is \",\n",
    "                pipeline_kwargs=dict(max_new_tokens=20))"
   ],
   "id": "2e432be976783dcd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky is 120 feet above the base of a building. From a point on the ground, the angle'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:59:00.698045Z",
     "start_time": "2025-02-06T12:58:52.116901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chat model with BaseLLM\n",
    "# The same result as the below\n",
    "user_input = \"what is 1+1 equal to ? explain why in shortly.\"\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"}, # automatically added\n",
    "    {\"role\": \"users\", \"content\": user_input},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# Langchan_HF does not automatically apply_chat_template, it just str2str. So we have to do it.\n",
    "# Langchan_HF let pipeline call apply_chat_template, but pipeline only call it when input not a string or list[str]. Also it does\n",
    "# not call tools as argument. So in general, it does not call apply_chat_template\n",
    "\n",
    "# Because we pass a string, a string MUST BE generated through apply_chat_template first. Otherwise, we can just mass the messages (list[dict])\n",
    "print(pipeline.invoke(\n",
    "    prompt,\n",
    "    pipeline_kwargs=dict(top_k=1,\n",
    "                         return_full_text=False)\n",
    "))\n",
    "print(\"===========================================================================\")\n",
    "# HuggingFacePipeline does not bind_tools to template. So we have to call it by templated input with tools.\n",
    "# But HuggingFacePipeline.invoke result a string. Not reserve a chat structure.\n",
    "# We we pass a structure, it will convert to a string then pass to Pipeline. So Pipeline don't apply chat template."
   ],
   "id": "5c2a63b7b1801c49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of one plus one equals two.\n",
      "\n",
      "In mathematics, addition is the operation of combining two or more numbers into a single number. When you add one to another one, you get a total of two. This can be represented as:\n",
      "\n",
      "1 + 1 = 2\n",
      "\n",
      "This simple equation demonstrates the basic principle of counting and quantity. It's a fundamental concept that forms the basis for many other mathematical operations and calculations.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T02:40:36.491159Z",
     "start_time": "2025-02-03T02:40:27.400122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Both have the same result\n",
    "# HF pipeline will automatically apply_chat_template with add_generation_prompt = True\n",
    "print(hf_pipe(messages, top_k=1)[0]['generated_text'][-1]['content'])\n",
    "print(\"===========================================================================\")"
   ],
   "id": "be9b9049769dacb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of one plus one equals two.\n",
      "\n",
      "In mathematics, addition is the operation of combining two or more numbers into a single number. When you add one to another one, you get a total of two. This can be represented as:\n",
      "\n",
      "1 + 1 = 2\n",
      "\n",
      "This simple equation demonstrates the basic principle of counting and quantity. It's a fundamental concept that forms the basis for many other mathematical operations and calculations.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T09:28:23.248715Z",
     "start_time": "2025-02-02T09:28:23.245162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# Try this for more details about use tool\n",
    "# tokenizer.chat_template()\n",
    "# tokenizer.get_chat_template(None,tools)"
   ],
   "id": "394706e571cc7a07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nwhat is 1+1 equal to ? explain why in shortly.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chat models\n",
    "Chat model (BaseChatModel) interface enables back and forth conversations between the user and model.\n",
    "\n",
    "The reason why itâ€™s a separate interface is that popular LLM providers like OpenAI differentiate messages sent to and from the model into\n",
    "user, assistant, and system roles. Chat models also offer addition capabilities:\n",
    "1. Tool calling (role tool)\n",
    "2. Structured Output\n",
    "3. Multimodality"
   ],
   "id": "79e1593cc746b937"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Message\n",
    "Messages are the unit of communication in chat models. Contain:\n",
    "1. Role\n",
    "2. Content\n",
    "3. Others: ID, name, metadata, tool calls,...\n",
    "\n",
    "Messages are base on BaseMessage, including:\n",
    "1. SystemMessage\n",
    "2. HumanMessage\n",
    "3. AIMessage, AIMessageChunk\n",
    "4. ToolMessage\n",
    "5. Multimodality\n"
   ],
   "id": "1b476221fcd27c75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:59:21.664717Z",
     "start_time": "2025-02-06T12:59:11.965250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ChatHuggingFace is BaseChatModel\n",
    "chat_model = ChatHuggingFace(llm=pipeline)\n",
    "\n",
    "prompt = [HumanMessage(user_input), ]\n",
    "\n",
    "# chat_model.invoke DOES call apply_chat_template with add_generation_prompt = True,\n",
    "# so the result is the same as above and we don't need to call apply_chat_template like LLMs.\n",
    "print(chat_model.invoke(prompt,\n",
    "                        pipeline_kwargs=dict(top_k=1)).content\n",
    "      )"
   ],
   "id": "f7cb2494b2342910",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "what is 1+1 equal to ? explain why in shortly.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The sum of one plus one equals two.\n",
      "\n",
      "In mathematics, addition is the operation of combining two or more numbers into a single number. When you add one to another one, you get a total of two. This can be represented as:\n",
      "\n",
      "1 + 1 = 2\n",
      "\n",
      "This simple equation demonstrates the basic principle of counting and quantity. It's a fundamental concept that forms the basis for many other mathematical operations and calculations.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process internal\n",
    "cvt_input = chat_model._convert_input(\"tell me a joke\").to_messages()\n",
    "print(cvt_input,end='\\n\\n')\n",
    "llm_input = chat_model._to_chat_prompt(cvt_input)\n",
    "print(llm_input,end='\\n\\n')\n",
    "llm_result = pipeline._generate(prompts=[llm_input])\n",
    "print(llm_result,end='\\n\\n\\n')\n",
    "# Furthur processing\n"
   ],
   "id": "79e0f584437ca71b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompt template",
   "id": "ea4df6ba4a9fe7bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:59:58.065964Z",
     "start_time": "2025-02-06T12:59:58.061165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The system role in the first message will be set to default content (through apply chat template) if not provide.\n",
    "# If provided, it will override the default\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer and explain the question based on the context.'),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "context = 'A 5 years old child is learning math and ask the question. Need to explain for the kid to understand.'\n",
    "question = 'what is 3+5 ?'\n",
    "ques_prompt = template.invoke(dict(context=context, question=question))\n",
    "print(ques_prompt)"
   ],
   "id": "6b225e4cb3bed1be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer and explain the question based on the context.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Context: A 5 years old child is learning math and ask the question. Need to explain for the kid to understand.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: what is 3+5 ?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:11:37.174327Z",
     "start_time": "2025-02-03T06:11:28.501803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chat_model.invoke(ques_prompt,\n",
    "                        pipeline_kwargs=dict(return_full_text=False,\n",
    "                                             top_k=1)\n",
    "                        ).content)"
   ],
   "id": "3612e1dcca9486d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 8.\n",
      "Explanation: When you add two numbers together, you combine their values. In this case, you're adding 3 and 5. You can think of it as counting up from 3 by 5 steps or starting at 5 and counting up by 3 steps. Either way, you end up with a total of 8. So, 3 + 5 equals 8.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T03:18:18.729591Z",
     "start_time": "2025-02-03T03:18:15.503993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The output is AIMessage but cover both prompt and answer.\n",
    "chat_model.invoke(ques_prompt)"
   ],
   "id": "3a43a82e43781672",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|im_start|>system\\nAnswer and explain the question based on the context.<|im_end|>\\n<|im_start|>user\\nContext: A 5 years old child is learning math and ask the question. Need to explain for the kid to understand.<|im_end|>\\n<|im_start|>user\\nQuestion: what is 3+5 ?<|im_end|>\\n<|im_start|>assistant\\nThe answer is 8. Three plus five makes eight because you count up by fives from three, which gives you a total of eight.', additional_kwargs={}, response_metadata={}, id='run-c6bb1e4d-5ab1-4b30-8085-fd4be7fde2b4-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T03:16:40.059189Z",
     "start_time": "2025-02-03T03:16:33.231408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# using raw string with chat model, chat model will automatically convert to chat template\n",
    "print(chat_model.invoke(\"The sea is \").content)"
   ],
   "id": "bf4178cfbe34aaef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "The sea is <|im_end|>\n",
      "<|im_start|>assistant\n",
      "the vast expanse of water that covers most of Earth's surface. It plays a crucial role in regulating the planet's climate and supporting marine life. The sea contains diverse ecosystems ranging from coral reefs to deep-sea trenches and has been instrumental in shaping human history through trade routes, transportation, and natural resources like fish and oil.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Output parser\n",
    "* There two ways to parse output in structured format like json or pydantic:\n",
    "1. Built in with_structured_output()\n",
    "2. Using OutputParser like PydanticOutputParser, Custom Parsing, JsonOutputParser. Parser will instruct model generate format via prompt, then\n",
    "parse that output into json.\n",
    "\n",
    "* with_structured_output() is implemented for models that provide native APIs for structuring outputs.\n",
    "* Not all chat model support structure output, so below will return None when invoke.\n",
    "* Find supported structure output model in : # https://python.langchain.com/docs/integrations/chat/\n",
    "* Steps for create parser:\n",
    "1. Define schema.\n",
    "2. Inject instruction from parser to prompt.\n",
    "3. Append parser as last of chain.\n",
    "\n",
    "Note:\n",
    "Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON."
   ],
   "id": "112a1a265f96c1a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Json\n",
    "\n",
    "\n"
   ],
   "id": "df41c8a8a346a43e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T15:24:12.337951Z",
     "start_time": "2025-02-06T15:24:09.168819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from transformers import TextStreamer\n",
    "from langchain_core.tools import tool\n",
    "from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# you can pass streamer when create pipeline\n",
    "streamer = TextStreamer(tokenizer=tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "joke_prompt = [HumanMessage(\"tell me a joke, return json format with 'setup' and 'punchline' keys .\"), ]\n",
    "\n",
    "\n",
    "class JokeOutput(BaseModel):\n",
    "    \"\"\"A joke format\"\"\"\n",
    "    setup: str = Field(description='question to setup a joke')\n",
    "    punchline: str = Field(description='answer to resolve the joke and make us laugh.')\n",
    "\n",
    "# JokeOutput = {\n",
    "#     'setup' :'question to setup a joke' ,\n",
    "#     'punchline':  'answer to resolve the joke and make us laugh.'\n",
    "# }\n",
    "#\n",
    "structured_chat = chat_model.bind_tools([JokeOutput])\n",
    "tool_chat = chat_model.bind_tools([multiply], tool_choice = 'auto')\n",
    "\n",
    "\n",
    "# Because Bullshiet langchain set toolchoice = 'any' but bind_tools in ChatHuggingFace = 'auto' or 'none'\n",
    "# structured_chat.first.kwargs['tool_choice'] =  structured_chat.first.kwargs['tools'][0]\n",
    "joke = structured_chat.invoke(joke_prompt,\n",
    "                             pipeline_kwargs=dict(\n",
    "                                 return_full_text=False,\n",
    "                                 top_k=1,\n",
    "                                 streamer=streamer\n",
    "    ))\n",
    "type(joke)"
   ],
   "id": "8a3bde3c6672adc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"setup\": \"Why don't scientists trust atoms?\",\n",
      "  \"punchline\": \"Because they make up everything!\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T15:24:16.687170Z",
     "start_time": "2025-02-06T15:24:16.680225Z"
    }
   },
   "cell_type": "code",
   "source": "joke",
   "id": "41bb1282201304f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"setup\": \"Why don\\'t scientists trust atoms?\",\\n  \"punchline\": \"Because they make up everything!\"\\n}', additional_kwargs={}, response_metadata={}, id='run-7730852f-120c-4bd3-8226-4f15030285ae-0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T15:24:23.726948Z",
     "start_time": "2025-02-06T15:24:22.223124Z"
    }
   },
   "cell_type": "code",
   "source": "tool_chat.invoke(\"what is 3*12\")",
   "id": "e2ad0a92dc8d7bd0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nwhat is 3*12<|im_end|>\\n<|im_start|>assistant\\nThe product of 3 and 12 is 36.', additional_kwargs={}, response_metadata={}, id='run-8f8cb2ff-769d-4a05-bc6b-f2b7ebd6c43e-0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:13:21.903703Z",
     "start_time": "2025-02-03T06:13:21.901304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=JokeOutput)\n",
    "print(parser.get_format_instructions())"
   ],
   "id": "d985aec1b1635172",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"description\": \"question to setup a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke and make us laugh.\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:13:24.176818Z",
     "start_time": "2025-02-03T06:13:24.172016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template2 = ChatPromptTemplate.from_messages([\n",
    "    # The first system message is not set, so the default is used when apply_chat_template.\n",
    "    # ('system', 'Answer and explain the ques.'), #\n",
    "    ('human', 'Instruction: {context}'),\n",
    "    ('human', 'Command: {question}'),\n",
    "])\n",
    "ques_prompt2 = template2.invoke(dict(context=parser.get_format_instructions(), question=\"Tell me a joke\"))\n",
    "print(ques_prompt2)\n"
   ],
   "id": "44fb8d29d78e2ab0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Instruction: The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"description\": \"question to setup a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke and make us laugh.\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='Command: Tell me a joke', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:13:29.675626Z",
     "start_time": "2025-02-03T06:13:26.708214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# chain = chat_model|parser\n",
    "# chain.invoke(ques_prompt2)\n",
    "chat_out = chat_model.invoke(ques_prompt2,\n",
    "                             pipeline_kwargs=dict(top_k=1, return_full_text=False))"
   ],
   "id": "96eb686ac5961baf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:13:49.634786Z",
     "start_time": "2025-02-03T06:13:49.632341Z"
    }
   },
   "cell_type": "code",
   "source": "print(chat_out)",
   "id": "1852809d39614ecd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{\\n  \"setup\": \"Why don\\'t scientists trust atoms?\",\\n  \"punchline\": \"Because they make up everything!\"\\n}' additional_kwargs={} response_metadata={} id='run-3e49a4ec-790c-4487-9d17-f9e1609bc3f8-0'\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:14:07.954178Z",
     "start_time": "2025-02-03T06:14:07.951154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_out = parser.invoke(chat_out)\n",
    "json_out"
   ],
   "id": "d336a6f1f691e1b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RunnableInterface\n",
    "1. invoke\n",
    "2. batch\n",
    "3. stream"
   ],
   "id": "4bdc0cce91f7f28b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Streaming",
   "id": "d4715595d70f75ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Streamer",
   "id": "6bf785017608f1e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:14:59.189398Z",
     "start_time": "2025-02-03T07:14:50.335225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Streamer from HF\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# you can pass streamer when create pipeline\n",
    "streamer = TextStreamer(tokenizer=tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "a = chat_model.invoke(\n",
    "    ques_prompt,\n",
    "    pipeline_kwargs=dict(\n",
    "        return_full_text=False,\n",
    "        top_k=1,\n",
    "        streamer=streamer\n",
    "    ))"
   ],
   "id": "a674662d301ca289",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 8.\n",
      "Explanation: When you add two numbers together, you combine their values. In this case, you're adding 3 and 5. You can think of it as counting up from 3 by 5 steps or starting at 5 and counting up by 3 steps. Either way, you end up with a total of 8. So, 3 + 5 equals 8.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TextIteratorStreamer",
   "id": "c5a5a2dc4c3d7e19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:29:58.634206Z",
     "start_time": "2025-02-03T07:29:49.951358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "from sys import stdout\n",
    "\n",
    "istreamer = TextIteratorStreamer(tokenizer=tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "thread = Thread(\n",
    "    target=chat_model.invoke,\n",
    "    args=(ques_prompt,),\n",
    "    kwargs=dict(\n",
    "        pipeline_kwargs=dict(\n",
    "            return_full_text=False,\n",
    "            top_k=1,\n",
    "            streamer=istreamer\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "thread.start()\n",
    "print(\"Thread started\")\n",
    "gen_text = \"\"\n",
    "for text in istreamer:\n",
    "    gen_text += text\n",
    "    # print(text,end=\"\")\n",
    "    stdout.write(text)\n",
    "    stdout.flush()"
   ],
   "id": "cfd3fb7e8581c92c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread started\n",
      "The answer is 8.\n",
      "Explanation: When you add two numbers together, you combine their values. In this case, you're adding 3 and 5. You can think of it as counting up from 3 by 5 steps or starting at 5 and counting up by 3 steps. Either way, you end up with a total of 8. So, 3 + 5 equals 8."
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AsyncTextIteratorStreamer",
   "id": "4487d2d997580cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T08:22:09.839409Z",
     "start_time": "2025-02-03T08:22:09.816521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AsyncTextIteratorStreamer\n",
    "import asyncio as aio\n",
    "\n",
    "\n",
    "async def delay(t=0.5):\n",
    "    await aio.sleep(t)\n",
    "\n",
    "\n",
    "async def main(s, t):\n",
    "\n",
    "    aio.create_task(delay(t))\n",
    "    async for text in s:\n",
    "        # print(text,end=\"\")\n",
    "        stdout.write(text)\n",
    "        stdout.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aistreamer = AsyncTextIteratorStreamer(tokenizer=tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    thread = Thread(\n",
    "        target=chat_model.invoke,\n",
    "        args=(ques_prompt,),\n",
    "        kwargs=dict(\n",
    "            pipeline_kwargs=dict(\n",
    "                return_full_text=False,\n",
    "                top_k=1,\n",
    "                streamer=aistreamer\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    thread.start()\n",
    "    print(\"Thread started\")\n",
    "\n",
    "    aio.run(main(aistreamer,1))"
   ],
   "id": "6af275cfa2c6f41d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread started\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 33\u001B[0m\n\u001B[1;32m     30\u001B[0m thread\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThread started\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m \u001B[43maio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43maistreamer\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.12/asyncio/runners.py:190\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(main, debug, loop_factory)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03mThis function runs the passed coroutine, taking care of\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m    asyncio.run(main())\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;66;03m# fail fast with short traceback\u001B[39;00m\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Runner(debug\u001B[38;5;241m=\u001B[39mdebug, loop_factory\u001B[38;5;241m=\u001B[39mloop_factory) \u001B[38;5;28;01mas\u001B[39;00m runner:\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mrun(main)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tools",
   "id": "238d8ed7c3b976cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "# ChatHuggingFace does not call apply_chat_template with tools as parameters. So we have to apply manually and can not just bind_tools().\n",
    "#"
   ],
   "id": "43fb4e9b7752a434"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:30:28.459845Z",
     "start_time": "2025-02-07T16:30:28.418862Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import TextGenerationPipeline, Text2TextGenerationPipeline",
   "id": "4bec65d54dcb66fb",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m T5Tokenizer\n\u001B[0;32m----> 2\u001B[0m t \u001B[38;5;241m=\u001B[39m \u001B[43mT5Tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/projects/chatbone/.venv/lib/python3.12/site-packages/transformers/utils/dummy_sentencepiece_objects.py:226\u001B[0m, in \u001B[0;36mT5Tokenizer.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 226\u001B[0m     \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msentencepiece\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/projects/chatbone/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1678\u001B[0m, in \u001B[0;36mrequires_backends\u001B[0;34m(obj, backends)\u001B[0m\n\u001B[1;32m   1676\u001B[0m failed \u001B[38;5;241m=\u001B[39m [msg\u001B[38;5;241m.\u001B[39mformat(name) \u001B[38;5;28;01mfor\u001B[39;00m available, msg \u001B[38;5;129;01min\u001B[39;00m checks \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m available()]\n\u001B[1;32m   1677\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failed:\n\u001B[0;32m-> 1678\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(failed))\n",
      "\u001B[0;31mImportError\u001B[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_huggingface import HuggingFaceEndpoint",
   "id": "a331271d3a39e08b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
