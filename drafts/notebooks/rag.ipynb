{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ref: https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x\n",
    "* Steps:\n",
    "2. Indexing\n",
    "2. Retrieval\n",
    "3. Generation\n",
    "\n",
    "* Consider:\n",
    "\n",
    "How do we handle the variability in the quality of a user’s input?: Query transformation\n",
    "\n",
    "How do we route queries to retrieve relevant data from a variety of data sources?\n",
    "\n",
    "How do we transform natural language to the query language of the target data source?\n",
    "\n",
    "How do we optimize our indexing process, i.e., embedding, text splitting?"
   ],
   "id": "db440b06bbf70ed4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Query Processor",
   "id": "7e78c0c06c48312e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Query transformation\n",
    "Problem in RAG is relying too heavily on the quality of a user’s query to generate an accurate output.Query transformation is a subset of strategies\n",
    " designed to modify the user’s input to answer the first RAG problem question: How do we handle the variability in the quality of a user’s input.\n",
    "\n",
    "Make a user’s input more or less abstract in order to generate an accurate LLM output.\n",
    "\n",
    "\n",
    "Steps:\n",
    "1. Rewrite\n",
    "\n",
    "> Removing irrelevant/unrelated text from the query.\n",
    "\n",
    "> Grounding the query with past conversation history. For instance, to make sense of a query such as and what about in LA, we need to combine it with a hypothetical past question about the weather in SF, to arrive at a useful query such as weather in LA.\n",
    "\n",
    "> Casting a wider net for relevant documents by also fetching documents for related queries.\n",
    "\n",
    "> Decomposing a complex question into multiple, simpler questions and then including results for all of them in the final prompt to generate an answer.\n",
    "\n",
    "\n",
    "2. Combine\n",
    "\n",
    "Strategy:\n",
    "1. Rewrite-Retrieve-Read: prompts the LLM to rewrite the user’s query before performing retrieval.\n",
    "2. Multi-Query Retrieval:instructing an LLM to generate multiple queries based on a user’s initial query, then use all of them to retrieving.\n",
    "3. RAG-Fusion: The same as multi-query retrieval, except that we apply a final reranking step to all the retrieved documents using reciprocal rank fusion\n",
    "(RRF) algorithm.\n",
    "4. Hypothetical Document Embeddings (HyDE): Creating a hypothetical document based on the user’s query, embedding the document, and retrieving.\n",
    "The intuition is that an LLM-generated hypothetical document will be more similar to the most relevant documents than the original query."
   ],
   "id": "be72f8f88325263a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Query rounting\n",
    "Required data may live in a variety of data sources. We would like to route the query to the appropriate inferred data source to retrieve relevant docs.\n",
    "\n",
    "Query routing is a strategy used to forward a user’s query to the relevant data source. Or adapter of reasoning model.\n",
    "\n",
    "1. Logical routing: give the LLM knowledge of the various data sources at our disposal and then let the LLM reason which data source to apply\n",
    " based on the user’s query. Use LLM to classify each query into one of the available routes.\n",
    "2. Semantic routing involves pre embedding various prompts (template) that represent various data sources alongside the user’s query and then performing\n",
    "vector similarity search to retrieve the most similar prompt.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9abd55906de7eb2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Query Construction\n",
    "Query construction is the process of transforming a natural language query into the query language of the database or data source you are interacting with.\n",
    "1. Text-to-Metadata Filter (SelfQueryRetriever): Most vector stores provide the ability to limit your vector search based on metadata.  During the\n",
    "embedding process, we can attach metadata key-value pairs to vectors in an index and then later specify filter expressions when you query the index.\n",
    "\n",
    "We have to describe which fields the metadata of our documents contain; that description will be included in the prompt.\n",
    "The retriever will then do the following:\n",
    "* Send the query generation prompt to the LLM.\n",
    "\n",
    "* Parse metadata filter and rewritten search query from the LLM output.\n",
    "\n",
    "* Convert the metadata filter generated by the LLM to the format appropriate for our vector store.\n",
    "\n",
    "*  Issue a similarity search against the vector store, filtered to only match documents whose metadata passes the generated filter.\n",
    "\n",
    "2. Text-to-SQL: LLM must be provided with an accurate description of the database\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2baab356fc90ae1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
